{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Content Pipeline","text":""},{"location":"#automated-content-creation-at-scale","title":"Automated Content Creation at Scale","text":"<p>The Content Pipeline is a fully automated system that transforms trending social media topics into publish-ready short-form video content. From identifying what's trending to producing a polished, captioned video with branded overlays \u2014 the entire process is orchestrated end-to-end with minimal manual intervention.</p>"},{"location":"#what-this-system-does","title":"What This System Does","text":"<p>At its core, the pipeline answers a simple question: \"What's trending right now, and how do we turn that into a video \u2014 fast?\"</p> <p>It handles every step of the process:</p> <ol> <li>Discovers what topics are gaining traction across social platforms</li> <li>Writes an informed, research-backed narration script</li> <li>Personalizes that script to match a specific account's voice and style</li> <li>Produces a full video with a talking-head presenter and supporting visuals</li> <li>Enhances the video with branded logo animations and word-by-word captions</li> <li>Publishes the finished video across multiple platforms</li> </ol> <p>What would normally require a content researcher, scriptwriter, video editor, and social media manager is handled by a single automated pipeline.</p>"},{"location":"#why-this-exists","title":"Why This Exists","text":"<p>Before this system, content production was a manual, time-intensive process. Each video required hours of research, writing, editing, and formatting \u2014 and that was for a single account. Scaling to multiple accounts with different voices and styles was simply not feasible by hand.</p> <p>This pipeline was built to solve that problem. It takes the entire workflow \u2014 from trend discovery to finished video \u2014 and automates it into a repeatable, scalable process.</p>"},{"location":"#system-architecture","title":"System Architecture","text":""},{"location":"#how-this-documentation-is-organized","title":"How This Documentation Is Organized","text":"Section What It Covers Pipeline Overview The big picture \u2014 how all the stages connect from start to finish Pipeline Stages (1\u20137) A dedicated page for each stage of the pipeline, explaining what it does and why it matters Multi-Account Scaling How the system handles multiple accounts with different voices, styles, and audiences Technology Stack A summary of the services and tools that power the pipeline <p>A Living Document</p> <p>This documentation will continue to evolve. Future revisions will include visual flowcharts, diagrams, and a history section covering the original manual process that this pipeline replaced.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"knowledge-base/","title":"Knowledge Base","text":""},{"location":"knowledge-base/#the-data-behind-the-scripts","title":"The Data Behind the Scripts","text":"<p>Every script the pipeline produces is backed by a curated knowledge base \u2014 a structured collection of technical articles, academic research papers, and narrative source documents. This isn't a random scrape of the internet. Each piece of content was deliberately selected, organized, and indexed to give the script generation stage the depth it needs.</p> <p>This page provides an overview of what's currently indexed.</p>"},{"location":"knowledge-base/#at-a-glance","title":"At a Glance","text":"Category Count Description Technical Articles 47 In-depth coverage of core technology, features, and infrastructure Research Papers 5 Peer-reviewed academic papers published on ArXiv Narrative Documents 17 Industry context, policy remarks, and broader market framing Improvement Proposals 136 Official protocol improvement proposals (AIPs)"},{"location":"knowledge-base/#technical-articles-47","title":"Technical Articles (47)","text":"<p>The backbone of the knowledge base. These articles cover the full technology stack, organized by topic area.</p>"},{"location":"knowledge-base/#core-consensus-execution","title":"Core Consensus &amp; Execution","text":"<p>Articles covering how the underlying blockchain achieves its performance characteristics \u2014 parallel execution, consensus protocols, latency reduction, and sharded scaling.</p> <ul> <li>Block-STM \u2014 Parallel execution engine (160K+ TPS)</li> <li>Quorum Store \u2014 Horizontal consensus scaling</li> <li>Shoal \u2014 DAG-based latency reduction</li> <li>Consensus Observer \u2014 Ultra low-latency sync</li> <li>Velociraptr \u2014 40% faster block times</li> <li>Zaptos \u2014 Sub-second latency architecture</li> <li>Shardines \u2014 Sharded execution (1M+ TPS)</li> </ul>"},{"location":"knowledge-base/#mev-privacy","title":"MEV &amp; Privacy","text":"<p>Articles on frontrunning protection and transaction privacy.</p> <ul> <li>Encrypted Mempool \u2014 Native MEV protection</li> <li>Confidential Transactions \u2014 Private balances and amounts</li> </ul>"},{"location":"knowledge-base/#move-language-vm","title":"Move Language &amp; VM","text":"<p>Articles covering the smart contract language, its evolution, and virtual machine performance.</p> <ul> <li>Loader V2 \u2014 2x VM performance improvement</li> <li>Move in 2025 \u2014 Language evolution roadmap</li> <li>Move Language Overview \u2014 Core concepts and design philosophy</li> <li>Lazy Loading \u2014 Composable dependency management</li> <li>Higher-Order Move \u2014 Function values and advanced patterns</li> <li>Move Security \u2014 Security guidelines and best practices</li> </ul>"},{"location":"knowledge-base/#account-transaction-innovations","title":"Account &amp; Transaction Innovations","text":"<p>Articles on wallet, account, and transaction model innovations.</p> <ul> <li>Stateless Accounts \u2014 Orderless transaction processing</li> <li>Account Abstraction \u2014 Flexible authentication models</li> <li>X-Chain Accounts \u2014 Cross-chain wallet support</li> <li>Keyless Accounts \u2014 Social login integration</li> <li>Event-Driven Transactions \u2014 Native on-chain automation</li> </ul>"},{"location":"knowledge-base/#developer-features","title":"Developer Features","text":"<p>Articles on SDKs, token standards, and developer tooling.</p> <ul> <li>Fungible Asset Standard \u2014 Modern token standard</li> <li>Dynamic Script Composer \u2014 Multi-call transaction batching</li> <li>TypeScript SDK \u2014 Developer SDK overview</li> <li>Move Objects \u2014 Object primitives and data model</li> <li>Randomness API \u2014 On-chain verifiable random function</li> <li>Transaction Simulation \u2014 Network forking for testing</li> </ul>"},{"location":"knowledge-base/#infrastructure","title":"Infrastructure","text":"<p>Articles on system architecture, APIs, staking, and gas economics.</p> <ul> <li>Infrastructure 2025 \u2014 Technical roadmap</li> <li>APIs Overview \u2014 REST, GraphQL, gRPC interfaces</li> <li>Standards \u2014 Token and protocol standards</li> <li>Architecture Deep Dive \u2014 Full system design</li> <li>Staking Guide \u2014 Validator staking operations</li> <li>Multidimensional Gas \u2014 Gas pricing research</li> </ul>"},{"location":"knowledge-base/#research-papers-5","title":"Research Papers (5)","text":"<p>Peer-reviewed academic papers that provide the theoretical foundations behind the technology.</p> Paper Focus Area Zaptos Minimal latency blockchain architecture Raptr Next-generation BFT consensus protocol Block-STM Parallel execution via optimistic concurrency Shoal DAG-based consensus optimization Narwhal-Tusk High-throughput mempool protocol"},{"location":"knowledge-base/#narrative-documents-17","title":"Narrative Documents (17)","text":"<p>These documents provide the broader context that gives scripts their narrative depth \u2014 industry history, policy positions, geopolitical framing, and market context.</p> Category Documents Founding Context Libra Whitepaper (v1 &amp; v2), Aptos Whitepaper, genesis and vision narratives Policy &amp; Regulation Treasury Secretary remarks on stablecoins, monetary policy perspectives Industry Analysis Stablecoin market analysis, institutional adoption narratives, mainstream adoption timelines Geopolitical Context Money as a strategic instrument, financial surveillance frameworks, global currency dynamics Institutional Narratives Key industry players, institutional involvement, ecosystem coordination <p>These documents are what allow the pipeline to produce scripts that go beyond surface-level reporting \u2014 connecting current events to deeper trends and historical context.</p>"},{"location":"knowledge-base/#improvement-proposals-136-aips","title":"Improvement Proposals (136 AIPs)","text":"<p>The complete set of official protocol improvement proposals. These track every significant change and feature addition to the protocol, providing a granular record of how the technology has evolved.</p> <p>Notable proposals indexed:</p> AIP Feature AIP-10 Move Objects AIP-41 Randomness API AIP-93 Consensus Observer AIP-106 Baby Raptr AIP-121 X-Chain Accounts (Ethereum) AIP-122 X-Chain Accounts (Solana) AIP-125 Event-Driven Transactions AIP-127 Lazy Loading AIP-131 Velociraptr"},{"location":"knowledge-base/#how-this-data-is-used","title":"How This Data Is Used","text":"<p>The knowledge base isn't just a static archive \u2014 it's actively queried during every script generation run. When a topic comes in, the system intelligently selects the most relevant articles, papers, and narrative documents to inform the script.</p> <p>Different topic categories pull from different parts of the knowledge base. A script about regulation will draw on policy documents and institutional narratives. A script about technical performance will pull from research papers and consensus articles. This ensures every script is grounded in the right context for its subject matter.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"multi-account/","title":"Multi-Account Scaling","text":""},{"location":"multi-account/#one-pipeline-many-voices","title":"One Pipeline, Many Voices","text":"<p>The Content Pipeline isn't designed for a single account \u2014 it's built from the ground up to support multiple accounts, each with its own distinct identity. The same trending topic can produce entirely different videos for different accounts, each one feeling authentically tailored to that account's audience.</p>"},{"location":"multi-account/#how-accounts-are-structured","title":"How Accounts Are Structured","text":"<p>Each account in the system is defined by a set of configuration files that capture its identity:</p> Component What It Defines Account Configuration Core metadata \u2014 the account's name, platform, and which voice and avatar to use for video production Personality Profile The account's voice and tone \u2014 how it speaks, its energy level, vocabulary, and mannerisms Video Style Guide How the account's videos are structured \u2014 hook timing, pacing, call-to-action format, and formatting preferences <p>Together, these files give the pipeline everything it needs to produce content that feels authentic to each account.</p>"},{"location":"multi-account/#what-changes-per-account","title":"What Changes Per Account","text":"<p>When the pipeline runs for a specific account, several stages adapt to that account's identity:</p>"},{"location":"multi-account/#script-personalization","title":"Script Personalization","text":"<p>The base script (which is the same regardless of account) gets rewritten to match the account's personality. A formal, analytical account will deliver the same information very differently than a casual, high-energy account.</p>"},{"location":"multi-account/#voice","title":"Voice","text":"<p>Each account has its own assigned voice for narration. The text-to-speech stage uses the account's specific voice configuration, so every video sounds like that account.</p>"},{"location":"multi-account/#visual-presenter","title":"Visual Presenter","text":"<p>Each account has its own AI avatar. The talking-head presenter in the video matches the account's assigned visual identity.</p>"},{"location":"multi-account/#output-isolation","title":"Output Isolation","text":"<p>Every account has its own run history. Outputs, intermediate files, and metadata are all organized by account \u2014 nothing bleeds across accounts.</p>"},{"location":"multi-account/#adding-a-new-account","title":"Adding a New Account","text":"<p>Scaling to a new account is a straightforward process:</p> <ol> <li>Create the account's directory with the required configuration files</li> <li>Define the personality \u2014 describe how this account talks, its tone, its mannerisms</li> <li>Define the video style \u2014 set the pacing, hook format, and structural preferences</li> <li>Assign a voice and avatar \u2014 configure which voice and presenter to use</li> </ol> <p>Once configured, the account is ready to receive content through the pipeline. No code changes are required \u2014 the pipeline reads the account's configuration at runtime and adapts automatically.</p>"},{"location":"multi-account/#running-across-accounts","title":"Running Across Accounts","text":"<p>The pipeline can process the same topic for multiple accounts sequentially. Each run is independent:</p> <ul> <li>Account A gets the topic \u2192 script is personalized in Account A's voice \u2192 video produced with Account A's avatar and voice \u2192 overlays and captions applied \u2192 finished video for Account A</li> <li>Account B gets the same topic \u2192 script is personalized in Account B's voice \u2192 video produced with Account B's avatar and voice \u2192 overlays and captions applied \u2192 finished video for Account B</li> </ul> <p>The base research and script generation only happen once. The personalization, production, and enhancement stages run independently per account.</p>"},{"location":"multi-account/#why-this-matters","title":"Why This Matters","text":"<p>Operating multiple social media accounts at scale is a massive operational challenge. Each account needs:</p> <ul> <li>Consistent voice and personality across every piece of content</li> <li>Unique delivery so accounts don't feel like copies of each other</li> <li>Independent output management and tracking</li> <li>The ability to cover the same topics without producing duplicate content</li> </ul> <p>Doing this manually would require a dedicated content creator for each account. This system handles it through configuration \u2014 each new account is a configuration file, not a new hire.</p> <p>Placeholder for Diagram</p> <p>A future revision will include a visual showing how a single topic flows through the pipeline for multiple accounts in parallel, producing distinct outputs.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"pipeline-overview/","title":"Pipeline Overview","text":""},{"location":"pipeline-overview/#from-trending-topic-to-published-video","title":"From Trending Topic to Published Video","text":"<p>The Content Pipeline is a multi-stage system where each stage feeds directly into the next. A trending topic enters the pipeline and \u2014 through a series of automated steps \u2014 exits as a fully produced, platform-ready video.</p> <p>This page provides the high-level view. Each stage has its own dedicated page with more detail.</p>"},{"location":"pipeline-overview/#the-stages-at-a-glance","title":"The Stages at a Glance","text":"Stage Name What Happens 1 Topic Discovery Scans social platforms for trending topics and surfaces the most relevant ones 2 Script Generation Writes an informed narration script backed by a curated knowledge base and live data 3 Script Personalization Rewrites the script to match a specific account's voice, tone, and style 4 Video Production Produces the full video \u2014 talking-head presenter, supporting visuals, and narration audio 5 Visual Overlays Adds branded logo animations that appear when relevant entities are mentioned 6 Captions Burns in word-by-word animated captions synced to the narration 7 Publishing Distributes the finished video across TikTok, YouTube Shorts, Instagram Reels, and Facebook Reels"},{"location":"pipeline-overview/#how-it-all-connects","title":"How It All Connects","text":"<p>The pipeline follows a strict sequential flow. Each stage produces an output that becomes the input for the next stage.</p>"},{"location":"pipeline-overview/#what-makes-this-complex","title":"What Makes This Complex","text":"<p>On the surface, \"turn a topic into a video\" sounds straightforward. In practice, it involves coordinating 7+ external services, managing dozens of intermediate files, and orchestrating processes that run in parallel to optimize for both speed and cost.</p> <p>A few things that set this system apart:</p> <ul> <li> <p>Research-Backed Scripts \u2014 Scripts aren't just generated from thin air. They're informed by a curated knowledge base of 50+ articles and research papers, combined with live market data pulled at generation time.</p> </li> <li> <p>Intelligent Video Rendering \u2014 The video production stage alone is a 12-step sub-pipeline that uses smart rendering optimization to dramatically reduce production costs while maintaining quality.</p> </li> <li> <p>Account-Aware Personalization \u2014 The same trending topic produces a different video for each account. Voice, tone, pacing, visual avatar, and narration style are all customized per account.</p> </li> <li> <p>Precision-Synced Enhancements \u2014 Logo overlays and captions aren't just slapped on. They're synced to the narration at the word level using audio transcription and intelligent timestamp matching.</p> </li> <li> <p>Fully Resumable \u2014 If any stage fails or needs adjustment, the pipeline can pick up exactly where it left off without re-running earlier stages.</p> </li> </ul>"},{"location":"pipeline-overview/#for-a-single-account","title":"For a Single Account","text":"<p>When the pipeline runs for one account, it follows the full sequence above. The result is a single finished video \u2014 personalized to that account's voice, enhanced with overlays and captions, and ready for publishing.</p> <p>Every run is tracked and stored with all intermediate outputs, so any stage can be reviewed, adjusted, or re-run independently.</p>"},{"location":"pipeline-overview/#for-multiple-accounts","title":"For Multiple Accounts","text":"<p>The pipeline is designed to scale across multiple accounts. Each account has its own identity \u2014 its own voice, personality, visual style, and audience. When the same trending topic is processed for different accounts, each one gets a uniquely personalized video.</p> <p>The multi-account architecture is covered in detail on the Multi-Account Scaling page.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"technology-stack/","title":"Technology Stack","text":""},{"location":"technology-stack/#what-powers-the-pipeline","title":"What Powers the Pipeline","text":"<p>The Content Pipeline integrates a wide range of services and technologies to handle every stage of the process \u2014 from trend discovery to finished video. This page provides a high-level summary of the technology landscape involved.</p>"},{"location":"technology-stack/#service-categories","title":"Service Categories","text":""},{"location":"technology-stack/#ai-language-models","title":"AI &amp; Language Models","text":"<p>AI is used throughout the pipeline for tasks that require understanding, generation, and creative decision-making:</p> <ul> <li>Script writing \u2014 Generating informed, research-backed narration scripts</li> <li>Topic extraction \u2014 Interpreting trending video content to identify actionable topics</li> <li>Creative decisions \u2014 Selecting visual positions, animation styles, and narrative angles</li> <li>Script personalization \u2014 Rewriting scripts to match account-specific voices</li> </ul>"},{"location":"technology-stack/#text-to-speech","title":"Text-to-Speech","text":"<p>Professional-quality voice synthesis converts written scripts into natural-sounding narration. Each account has its own assigned voice, ensuring distinct audio identity across accounts.</p>"},{"location":"technology-stack/#video-generation","title":"Video Generation","text":"<p>Two categories of video are generated in the pipeline:</p> <ul> <li>Talking-head presenter \u2014 AI-generated avatar that delivers the narration to camera</li> <li>Supporting visuals \u2014 Short video clips that illustrate concepts and entities mentioned in the script</li> </ul>"},{"location":"technology-stack/#audio-transcription","title":"Audio Transcription","text":"<p>Word-level audio transcription is used at multiple points in the pipeline:</p> <ul> <li>Synchronizing visual elements to the narration timeline</li> <li>Timing logo overlays to entity mentions</li> <li>Generating frame-accurate captions</li> </ul>"},{"location":"technology-stack/#motion-graphics-animation","title":"Motion Graphics &amp; Animation","text":"<p>Branded logo overlays are rendered using a dedicated animation engine capable of producing transparent, broadcast-quality motion graphics with multiple animation styles.</p>"},{"location":"technology-stack/#knowledge-base-retrieval","title":"Knowledge Base &amp; Retrieval","text":"<p>A curated collection of articles, research papers, and narrative documents is indexed and searchable. The system uses intelligent retrieval to surface relevant context for script generation, ensuring content is informed and accurate.</p>"},{"location":"technology-stack/#video-compositing","title":"Video Compositing","text":"<p>Professional video composition handles the assembly of all visual elements \u2014 presenter footage, supporting visuals, logo overlays, and captions \u2014 into a single, polished output.</p>"},{"location":"technology-stack/#content-distribution","title":"Content Distribution","text":"<p>The finished video is distributed across multiple social media platforms through a unified API integration.</p>"},{"location":"technology-stack/#scale-of-integration","title":"Scale of Integration","text":"<p>The pipeline coordinates 7+ external services across these categories, managing:</p> <ul> <li>Multiple API connections with different authentication methods</li> <li>Parallel job submission and polling across services</li> <li>Dozens of intermediate files per video produced</li> <li>Asset caching and reuse for cost optimization</li> <li>Per-account configuration across all service integrations</li> </ul>"},{"location":"technology-stack/#infrastructure","title":"Infrastructure","text":"<p>The pipeline runs as a Python-based orchestration system with:</p> <ul> <li>Modular stage architecture \u2014 each stage operates independently</li> <li>Resumable execution \u2014 can pick up from any stage if interrupted</li> <li>Run tracking \u2014 all outputs and metadata are preserved per run</li> <li>Account isolation \u2014 clean separation of configuration and outputs across accounts</li> </ul> <p>Intentional Abstraction</p> <p>This page provides a category-level overview of the technology involved. Specific service names, API configurations, and integration details are maintained separately from this documentation.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"pipeline/captions/","title":"Stage 6: Captions","text":""},{"location":"pipeline/captions/#word-by-word-animated-captions","title":"Word-by-Word Animated Captions","text":"<p>The final production stage adds animated, word-by-word captions synced precisely to the narration. These aren't static subtitles \u2014 they're styled, animated text overlays designed specifically for the short-form video format that audiences expect on platforms like TikTok.</p>"},{"location":"pipeline/captions/#how-it-works","title":"How It Works","text":""},{"location":"pipeline/captions/#transcription","title":"Transcription","text":"<p>The video's audio is transcribed with word-level precision, producing an exact timestamp for the start and end of every single word in the narration. This timestamp data is the foundation for syncing captions to speech.</p>"},{"location":"pipeline/captions/#word-grouping","title":"Word Grouping","text":"<p>Rather than flashing one word at a time (too fast to read) or showing full sentences (too much text on screen), the system groups words into 1\u20132 word display units. Short words are paired together when they fit naturally; longer words stand alone.</p> <p>This creates a reading rhythm that matches how the narration sounds \u2014 the captions feel like they're keeping pace with the speaker, not racing ahead or lagging behind.</p>"},{"location":"pipeline/captions/#styling","title":"Styling","text":"<p>The captions are styled to match the look and feel of professional TikTok content:</p> <ul> <li>Bold, clean font designed for readability on mobile screens</li> <li>High contrast \u2014 white text with a dark stroke so it's readable over any background</li> <li>Lower-third positioning \u2014 placed in the bottom portion of the screen where viewers naturally look for captions</li> <li>Word highlighting \u2014 the current word is visually emphasized to guide the viewer's eye</li> </ul>"},{"location":"pipeline/captions/#animation","title":"Animation","text":"<p>Each caption group animates in with a subtle scale effect \u2014 a quick pop-in that draws the eye without being distracting. This matches the fast-paced, dynamic feel that performs well on short-form platforms.</p>"},{"location":"pipeline/captions/#why-this-matters","title":"Why This Matters","text":"<p>Captions are no longer optional for short-form video. The majority of social media video is watched without sound, and even when sound is on, captions increase engagement and watch time. Platforms like TikTok actively favor captioned content in their algorithms.</p> <p>But adding captions manually \u2014 word by word, frame by frame, with animations and styling \u2014 is one of the most tedious parts of video editing. For a 30-second video, manually captioning can take 30\u201360 minutes.</p> <p>This stage produces broadcast-quality animated captions in minutes, fully synced to the narration with no manual timing adjustments needed.</p>"},{"location":"pipeline/captions/#what-comes-out","title":"What Comes Out","text":"<p>The output is the final video \u2014 complete with talking-head presenter, supporting visuals, branded logo overlays, and word-by-word animated captions. This is the finished product, ready for publishing.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"pipeline/output-showcase/","title":"Output Showcase","text":""},{"location":"pipeline/output-showcase/#what-the-pipeline-produces","title":"What the Pipeline Produces","text":"<p>This page showcases finished videos that were produced entirely by the Content Pipeline \u2014 from trending topic discovery through to the final captioned, branded output.</p> <p>Each video below went through every stage of the pipeline: topic discovery, script generation, personalization, video production, visual overlays, and captions.</p>"},{"location":"pipeline/output-showcase/#example-videos","title":"Example Videos","text":""},{"location":"pipeline/output-showcase/#video-1","title":"Video 1","text":"Your browser does not support the video tag."},{"location":"pipeline/output-showcase/#video-2","title":"Video 2","text":"Your browser does not support the video tag."},{"location":"pipeline/output-showcase/#video-3","title":"Video 3","text":"Your browser does not support the video tag."},{"location":"pipeline/output-showcase/#what-to-look-for","title":"What to Look For","text":"<p>When reviewing these videos, notice how the pipeline handles each layer:</p> <ul> <li>The hook \u2014 How the first few seconds grab attention, tailored to the account's style</li> <li>Presenter + visual transitions \u2014 The seamless alternation between the talking-head avatar and supporting b-roll visuals</li> <li>Logo overlays \u2014 Branded logos appearing at the exact moment an entity is mentioned, each with a context-appropriate animation style</li> <li>Captions \u2014 Word-by-word animated text synced precisely to the narration</li> <li>Voice and tone \u2014 How the account's personality comes through in the delivery</li> </ul> <p>Each of these elements was generated and assembled automatically by the pipeline.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"pipeline/publishing/","title":"Stage 7: Publishing","text":""},{"location":"pipeline/publishing/#distributing-content-across-platforms","title":"Distributing Content Across Platforms","text":""},{"location":"pipeline/publishing/#overview","title":"Overview","text":"<p>Once a video exits the pipeline as a finished product \u2014 with presenter, visuals, overlays, and captions \u2014 the final step is getting it in front of audiences.</p> <p>Publishing is handled through DoubleSpeed.ai, a platform that provides API access to upload and distribute video content across multiple social media platforms from a single integration point.</p> The DoubleSpeed.ai platform \u2014 centralized content distribution and automation."},{"location":"pipeline/publishing/#supported-platforms","title":"Supported Platforms","text":"<p>Through the DoubleSpeed API, finished videos are published to:</p> <ul> <li>TikTok</li> <li>YouTube Shorts</li> <li>Instagram Reels</li> <li>Facebook Reels</li> </ul> <p>This enables a single video to be distributed across all target platforms without manual uploading to each one individually.</p>"},{"location":"pipeline/publishing/#account-management-at-scale","title":"Account Management at Scale","text":"<p>DoubleSpeed provides a unified dashboard for managing all accounts in one place \u2014 tracking followers, views, engagement rates, and post status across every connected account.</p> The DoubleSpeed dashboard \u2014 managing multiple accounts with follower, view, and engagement tracking. <p>The infrastructure currently supports 30 connected accounts across platforms. The metrics below reflect an initial validation batch \u2014 a limited test distribution used to verify end-to-end pipeline delivery, confirm platform ingestion, and baseline engagement rates across account cohorts before scaling to full production volume.</p> Initial test batch metrics \u2014 30 accounts \u2014 6.4K views \u2014 4.4K impressions \u2014 230 engagements"},{"location":"pipeline/publishing/#production-throughput-target","title":"Production Throughput Target","text":"<p>With the pipeline validated and distribution infrastructure in place, the system is being tuned for a production cadence of 60 videos per day across the full account network. This involves optimizing batch scheduling, staggering publish windows for maximum algorithmic reach per platform, and balancing API rate limits across the account pool.</p> <p>The pipeline's modular architecture makes this scaling straightforward \u2014 topic discovery, script generation, and personalization already support multi-account throughput. The remaining optimization is in fine-tuning distribution timing and per-platform delivery parameters.</p>"},{"location":"pipeline/publishing/#how-it-fits-into-the-pipeline","title":"How It Fits Into the Pipeline","text":"<p>Publishing is the final stage of the content pipeline. It receives the fully produced video from Stage 6 (Captions) and handles the distribution logistics \u2014 uploading, platform-specific formatting, and scheduling.</p> <pre><code>Finished Video (from Stage 6)\n        |\n        v\n  +---------------+\n  |  DoubleSpeed   |  \u2500\u2500&gt;  TikTok\n  |     API        |  \u2500\u2500&gt;  YouTube Shorts\n  |                |  \u2500\u2500&gt;  Instagram Reels\n  |                |  \u2500\u2500&gt;  Facebook Reels\n  +---------------+\n</code></pre>"},{"location":"pipeline/publishing/#details-coming-soon","title":"Details Coming Soon","text":"<p>This section will be expanded to cover:</p> <ul> <li>How the DoubleSpeed integration works within the pipeline</li> <li>Platform-specific considerations</li> <li>Scheduling and timing of posts</li> <li>Multi-account publishing workflows</li> </ul> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"pipeline/script-generation/","title":"Stage 2: Script Generation","text":""},{"location":"pipeline/script-generation/#turning-a-topic-into-an-informed-script","title":"Turning a Topic into an Informed Script","text":"<p>Once a topic has been selected, the pipeline needs to produce a narration script \u2014 the words that will actually be spoken in the final video. But this isn't just \"write something about this topic.\" The scripts produced by this stage are research-backed, contextually aware, and designed for short-form video.</p>"},{"location":"pipeline/script-generation/#how-it-works","title":"How It Works","text":""},{"location":"pipeline/script-generation/#knowledge-base-retrieval","title":"Knowledge Base Retrieval","text":"<p>Every script is grounded in real information. The system maintains a curated knowledge base \u2014 a collection of technical articles, research papers, and narrative source documents that have been indexed and made searchable.</p> <p>When a topic comes in, the system queries this knowledge base to pull the most relevant context. The retrieval is intelligent \u2014 it doesn't just keyword-match. It understands the category of the topic and selects context that supports the right narrative angle.</p> <p>The knowledge base includes:</p> <ul> <li>50+ technical articles covering the subject matter in depth</li> <li>Published research papers from academic sources</li> <li>Narrative and contextual documents that provide broader framing</li> </ul> <p>This means the scripts aren't surface-level takes. They're informed by a body of knowledge that would take a human researcher significant time to assemble and reference.</p>"},{"location":"pipeline/script-generation/#live-data-integration","title":"Live Data Integration","text":"<p>In addition to the knowledge base, the system pulls live data at the time of script generation. This ensures that scripts reference current numbers and statistics rather than stale information.</p> <p>This combination of curated knowledge and real-time data gives each script both depth and freshness.</p>"},{"location":"pipeline/script-generation/#intelligent-topic-variation","title":"Intelligent Topic Variation","text":"<p>The system tracks what it has written before. When a new topic is similar to a previously covered one, the system recognizes this and adjusts its approach \u2014 pulling different context, taking a different narrative angle, and ensuring the output feels distinct.</p> <p>This prevents the content from becoming repetitive even when covering related subjects over time.</p>"},{"location":"pipeline/script-generation/#script-generation","title":"Script Generation","text":"<p>With the relevant context assembled, the system generates a narration script optimized for short-form video:</p> <ul> <li>30\u201340 seconds of spoken content</li> <li>Structured for engagement \u2014 hook, body, and call-to-action</li> <li>Conversational tone \u2014 written to be spoken, not read</li> <li>Factually grounded \u2014 claims backed by retrieved knowledge and live data</li> </ul>"},{"location":"pipeline/script-generation/#why-this-matters","title":"Why This Matters","text":"<p>Writing a good script for short-form video is harder than it looks. It needs to grab attention immediately, deliver value quickly, and feel natural when spoken aloud. Doing this consistently \u2014 while staying factually accurate and topically fresh \u2014 is a significant challenge at scale.</p> <p>This stage handles all of that automatically. Every script is informed by a substantial knowledge base, aware of what's been produced before, and structured specifically for the format it will be delivered in.</p>"},{"location":"pipeline/script-generation/#what-comes-out","title":"What Comes Out","text":"<p>The output is a base narration script \u2014 a clean, structured script ready to be personalized for a specific account in the next stage.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"pipeline/script-personalization/","title":"Stage 3: Script Personalization","text":""},{"location":"pipeline/script-personalization/#making-it-sound-like-them","title":"Making It Sound Like Them","text":"<p>The base script from Stage 2 is factually solid and well-structured \u2014 but it doesn't sound like anyone in particular. This stage takes that script and rewrites it to match the unique voice, tone, and style of a specific account.</p> <p>The facts stay the same. The delivery changes completely.</p>"},{"location":"pipeline/script-personalization/#how-it-works","title":"How It Works","text":""},{"location":"pipeline/script-personalization/#account-profiles","title":"Account Profiles","text":"<p>Every account in the system has a defined identity, captured in two profile documents:</p> <ul> <li> <p>Personality Profile \u2014 Describes how the account speaks. This includes tone, vocabulary, energy level, mannerisms, and the overall vibe. Think of it as a character sheet for the account's on-screen persona.</p> </li> <li> <p>Video Style Guide \u2014 Describes how the account's videos are structured. This covers things like hook timing, pacing, how the call-to-action is handled, and formatting preferences specific to the platform.</p> </li> </ul> <p>Together, these profiles give the system everything it needs to rewrite a script in a way that feels authentic to the account.</p>"},{"location":"pipeline/script-personalization/#the-rewrite","title":"The Rewrite","text":"<p>Using the account's personality and style profiles as a guide, the system rewrites the base script. This isn't just swapping a few words \u2014 it's a full rewrite of the delivery:</p> <ul> <li>Sentence structure adapts to the account's natural rhythm</li> <li>Word choice reflects how this account would actually talk</li> <li>Energy and pacing match the account's established style</li> <li>Hook and call-to-action follow the account's preferred patterns</li> </ul> <p>The core information \u2014 the facts, claims, and narrative \u2014 remains untouched. What changes is everything about how it's communicated.</p>"},{"location":"pipeline/script-personalization/#why-this-matters","title":"Why This Matters","text":"<p>This is what makes the multi-account model work. Without personalization, every account would sound the same \u2014 clearly automated and generic. With it, each account maintains a distinct, consistent identity that audiences recognize and connect with.</p> <p>It also means a single trending topic can produce multiple unique videos across different accounts. Same information, completely different feel. This is essential for operating multiple accounts without the content feeling duplicated or recycled.</p>"},{"location":"pipeline/script-personalization/#what-comes-out","title":"What Comes Out","text":"<p>The output is a personalized (flavored) script \u2014 rewritten in the account's voice, formatted to their style, and ready to be produced as a video in the next stage.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"pipeline/topic-discovery/","title":"Stage 1: Topic Discovery","text":""},{"location":"pipeline/topic-discovery/#finding-whats-trending-right-now","title":"Finding What's Trending Right Now","text":"<p>The pipeline begins by answering the most fundamental question in content creation: \"What should we talk about today?\"</p> <p>Rather than relying on guesswork or manually browsing social media, this stage automatically scans platforms for topics that are actively gaining traction \u2014 and surfaces the ones most worth covering.</p>"},{"location":"pipeline/topic-discovery/#how-it-works","title":"How It Works","text":""},{"location":"pipeline/topic-discovery/#scanning-for-trends","title":"Scanning for Trends","text":"<p>The system monitors TikTok for videos related to the content verticals we operate in. It looks across a defined set of hashtags and keywords that align with our focus areas, pulling in videos that have been posted within a recent time window.</p> <p>Not every video that's posted is worth paying attention to. The system applies engagement filters to narrow the results down to content that is actually resonating with audiences \u2014 filtering by view counts and other engagement signals to separate noise from genuine traction.</p>"},{"location":"pipeline/topic-discovery/#extracting-topics","title":"Extracting Topics","text":"<p>Raw video data isn't useful on its own. The system uses AI to analyze the top-performing videos and extract the actual topics being discussed. This isn't just keyword matching \u2014 it interprets the content to identify what the conversation is really about.</p> <p>Each extracted topic comes with context:</p> <ul> <li>What the topic is \u2014 a clear, actionable description</li> <li>What category it falls into \u2014 regulation, adoption, payments, institutional moves, etc.</li> <li>How confident the system is \u2014 based on how many videos are covering it and how much engagement they're generating</li> </ul>"},{"location":"pipeline/topic-discovery/#topic-selection","title":"Topic Selection","text":"<p>The extracted topics are ranked and presented for selection. This is one of the few points in the pipeline where a human makes a decision \u2014 choosing which topic to run through the rest of the pipeline.</p> <p>This intentional human checkpoint ensures the content stays strategically aligned, even though the discovery process itself is fully automated.</p>"},{"location":"pipeline/topic-discovery/#why-this-matters","title":"Why This Matters","text":"<p>Manual trend monitoring is slow and inconsistent. By the time someone manually identifies a trending topic, researches it, and starts scripting \u2014 the window of relevance may have already passed.</p> <p>This stage compresses what would be hours of manual monitoring into a process that runs in minutes. It ensures the pipeline is always working with topics that are timely, relevant, and proven to have audience interest.</p>"},{"location":"pipeline/topic-discovery/#what-comes-out","title":"What Comes Out","text":"<p>The output of this stage is a selected topic with its associated category. This feeds directly into Stage 2, where the topic becomes the foundation for a narration script.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"pipeline/video-production/","title":"Stage 4: Video Production","text":""},{"location":"pipeline/video-production/#the-most-complex-stage-in-the-pipeline","title":"The Most Complex Stage in the Pipeline","text":"<p>This is where a script becomes a video. What sounds simple on the surface \u2014 \"produce a video from a script\" \u2014 is actually a 12-step sub-pipeline that coordinates multiple services, generates dozens of intermediate assets, and uses intelligent optimization to keep production costs manageable.</p> <p>This stage produces a complete video with:</p> <ul> <li>A talking-head presenter (AI-generated avatar)</li> <li>Supporting b-roll visuals that appear during key moments</li> <li>Full narration audio synced to the presenter</li> <li>Smooth transitions between presenter and visual segments</li> </ul>"},{"location":"pipeline/video-production/#how-it-works","title":"How It Works","text":"<p>The 12 steps are grouped into four phases.</p>"},{"location":"pipeline/video-production/#phase-1-script-preparation","title":"Phase 1: Script Preparation","text":"<p>The personalized script needs to be prepared for video production. This involves two things:</p> <p>Cleaning the script \u2014 Removing any formatting or structural markers so the system is working with pure narration text.</p> <p>Marking visual positions \u2014 The system analyzes the script and identifies the moments where it makes sense to cut away from the presenter and show a supporting visual instead. These positions are marked with descriptions of what the visual should depict.</p> <p>This is an important creative decision \u2014 it determines the visual rhythm of the entire video. The system places 10\u201313 visual markers per script, creating a natural flow between the presenter talking directly to camera and supporting visuals that reinforce what's being said.</p>"},{"location":"pipeline/video-production/#phase-2-audio-timing","title":"Phase 2: Audio &amp; Timing","text":"<p>With the script prepared, the system generates and analyzes the narration audio:</p> <p>Narration generation \u2014 The clean script is sent to a text-to-speech service, which produces a high-quality spoken narration using the account's assigned voice. Each account has its own voice, so videos across different accounts sound distinct.</p> <p>Precision transcription \u2014 The generated audio is then transcribed back at the word level, producing exact timestamps for every single word spoken. This creates a precise timing map of the entire narration.</p> <p>Visual timing alignment \u2014 The visual markers from Phase 1 are matched against the word-level timestamps. Using intelligent matching, the system determines exactly when in the audio each visual should appear and for how long.</p> <p>This gives the system a frame-accurate timeline: it knows exactly which seconds need the presenter on screen and which seconds should show supporting visuals.</p>"},{"location":"pipeline/video-production/#phase-3-asset-generation","title":"Phase 3: Asset Generation","text":"<p>This is where the actual video assets are created. Two things happen in parallel to save time:</p> <p>Presenter rendering \u2014 The system renders the talking-head presenter. Rather than rendering the presenter for the entire video duration, the system uses intelligent rendering optimization \u2014 it only generates presenter footage for the specific moments where the presenter will actually be visible on screen.</p> <p>This is a critical cost and efficiency optimization. Presenter rendering is one of the most expensive operations in the pipeline, and this approach dramatically reduces both the time and cost required.</p> <p>Visual clip generation \u2014 While the presenter is being rendered, the system simultaneously generates the supporting visual clips. Each clip is created from the descriptions placed during script preparation. The system also maintains a visual asset library that allows it to reuse previously generated clips when appropriate, avoiding redundant generation.</p> <p>By running these two processes in parallel, the pipeline eliminates what would otherwise be a significant bottleneck.</p>"},{"location":"pipeline/video-production/#phase-4-final-assembly","title":"Phase 4: Final Assembly","text":"<p>With all the assets ready \u2014 presenter clips, visual clips, and the full narration audio \u2014 the system assembles the final video:</p> <ol> <li>The presenter clips are placed at their designated positions on the timeline</li> <li>The visual clips fill in the remaining segments</li> <li>The full narration audio is laid over the entire timeline</li> <li>Transitions are applied between segments for a polished look</li> </ol> <p>The result is a seamless video where the presenter appears to naturally alternate between speaking to camera and showing supporting visuals.</p>"},{"location":"pipeline/video-production/#why-this-matters","title":"Why This Matters","text":"<p>Video production is typically the most expensive and time-consuming part of content creation. This stage handles it entirely automatically \u2014 and does so in a way that's dramatically more cost-efficient than a naive approach would be.</p> <p>The key innovations here are:</p> <ul> <li>Intelligent rendering optimization that reduces the most expensive operation to a fraction of its full cost</li> <li>Parallel processing that eliminates bottleneck wait times</li> <li>Asset reuse that avoids redundant generation across videos</li> <li>Precision timing that ensures everything syncs at the frame level</li> </ul> <p>This is a 12-step process coordinating 4+ external services, dozens of intermediate files, and parallel job management \u2014 all fully automated.</p>"},{"location":"pipeline/video-production/#what-comes-out","title":"What Comes Out","text":"<p>The output is a complete video with talking-head presenter, supporting visuals, full narration audio, and transitions. This video then moves to Stage 5, where branded visual overlays are added.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"},{"location":"pipeline/visual-overlays/","title":"Stage 5: Visual Overlays","text":""},{"location":"pipeline/visual-overlays/#branded-logo-animations-timed-to-the-narration","title":"Branded Logo Animations, Timed to the Narration","text":"<p>When a video mentions a specific entity \u2014 a cryptocurrency, a company, a platform \u2014 this stage detects that mention and overlays an animated logo at exactly the right moment. It's a polished, professional touch that makes the content feel curated rather than auto-generated.</p>"},{"location":"pipeline/visual-overlays/#how-it-works","title":"How It Works","text":"<p>This stage runs through a 5-step process.</p>"},{"location":"pipeline/visual-overlays/#step-1-script-scanning","title":"Step 1: Script Scanning","text":"<p>The system scans the narration script for mentions of known entities. It works from a maintained asset library that maps keywords and aliases to logo assets.</p> <p>For example, if the script mentions \"Ripple\" or \"XRP\" or \"$XRP\" \u2014 all of these resolve to the same entity in the asset library. The scanning is smart enough to handle aliases, abbreviations, and common variations.</p> <p>To keep the video from feeling cluttered, the system limits how many times a single entity's logo can appear. If an entity is mentioned several times throughout the script, only the key appearances are selected.</p>"},{"location":"pipeline/visual-overlays/#step-2-timestamp-matching","title":"Step 2: Timestamp Matching","text":"<p>Once the entity mentions are identified, the system needs to know exactly when in the video they occur. Using word-level audio transcription, each mention is matched to a precise timestamp in the narration.</p> <p>This ensures the logo appears at the exact moment the entity is mentioned \u2014 not a second too early or too late.</p>"},{"location":"pipeline/visual-overlays/#step-3-animation-style-selection","title":"Step 3: Animation Style Selection","text":"<p>Not every logo should animate the same way. The system uses AI to select an animation style for each overlay based on the context \u2014 what entity is being mentioned, what's being said about it, and the emotional tone of the moment.</p> <p>Available animation styles include:</p> <ul> <li>Fade \u2014 Clean, subtle appearance</li> <li>Swing \u2014 Punchy snap-in with energy</li> <li>Bounce \u2014 Elastic overshoot for playful moments</li> <li>Zoom \u2014 Cinematic scale-in for dramatic reveals</li> <li>Shake \u2014 Impact effect for bold statements</li> </ul> <p>The style is chosen to complement the content, not distract from it.</p>"},{"location":"pipeline/visual-overlays/#step-4-overlay-rendering","title":"Step 4: Overlay Rendering","text":"<p>Each logo animation is rendered individually as a transparent overlay \u2014 meaning it can be placed on top of the video without blocking the underlying content. The rendering uses a dedicated animation engine that produces broadcast-quality motion graphics.</p>"},{"location":"pipeline/visual-overlays/#step-5-video-compositing","title":"Step 5: Video Compositing","text":"<p>The rendered overlays are composited onto the video at their designated timestamps. The result is a video where logos appear naturally alongside the narration, enhancing the professional feel of the content.</p>"},{"location":"pipeline/visual-overlays/#the-asset-library","title":"The Asset Library","text":"<p>The system maintains a curated library of logo assets organized by category:</p> <ul> <li>Cryptocurrencies \u2014 Major tokens and protocols with multiple logo variants</li> <li>Entities \u2014 Companies, platforms, and organizations referenced in content</li> </ul> <p>Each asset entry includes aliases (all the ways it might be mentioned in a script) and one or more logo files. The library is extensible \u2014 adding a new entity is as simple as adding an asset folder and updating the index.</p>"},{"location":"pipeline/visual-overlays/#why-this-matters","title":"Why This Matters","text":"<p>Logo overlays are a common feature in professional social media content, but doing them manually is tedious \u2014 you'd need to identify every mention, find the right logo, animate it, time it to the narration, and composite it. For a 30-second video, that's easily an hour of editing work.</p> <p>This stage does it all automatically, with frame-accurate timing and context-aware animation styles. It turns every video into something that looks hand-crafted by a professional editor.</p>"},{"location":"pipeline/visual-overlays/#what-comes-out","title":"What Comes Out","text":"<p>The output is the video with branded logo overlays added at all relevant moments. This feeds into Stage 6, where captions are applied as the final enhancement.</p> <p> ALGH \u2014 Content Pipeline Automation </p>"}]}